{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64684,"databundleVersionId":7098519,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import CategoricalAccuracy, MeanIoU\nimport matplotlib.pyplot as plt\n\n# Set your paths\ntrain_path = '/kaggle/input/cs770-final-project-skin-lesion-segmentation/Project_Data/Train'\n\n# Load ground truth information\nground_truth = pd.read_csv(os.path.join(train_path, 'GroundTruth.csv'))\n\n# Load Images, Masks, and Labels\nimages = [img_to_array(load_img(os.path.join(train_path, 'images', img + '.jpg'), target_size=(128, 128))) for img in ground_truth['image']]\nmasks = [img_to_array(load_img(os.path.join(train_path, 'masks', img + '_segmentation.png'), target_size=(128, 128), color_mode='grayscale')) for img in ground_truth['image']]\nlabels = ground_truth[['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']].values\n\n# Convert lists to numpy arrays\nimages = np.array(images) / 255.0  # Normalize pixel values to [0, 1]\nmasks = np.array(masks) / 255.0  # Normalize pixel values to [0, 1]\n\n# Split data into training and validation sets\nimages_train, images_val, masks_train, masks_val, labels_train, labels_val = train_test_split(\n    images, masks, labels, test_size=0.2, random_state=42\n)\n\n# Model for classification\ninput_shape = (128, 128, 3)  # Update this based on your image size\nnum_classes_classification = 7  # Multi-class classification\n\n# Classification input\ninputs_classification = Input(shape=input_shape, name='classification_input')\n\n# Encoder\nconv1_classification = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs_classification)\npool1_classification = MaxPooling2D(pool_size=(2, 2))(conv1_classification)\n\n# Classification branch\nflat_classification = Flatten()(pool1_classification)\ndense1_classification = Dense(128, activation='relu')(flat_classification)\noutputs_classification = Dense(num_classes_classification, activation='softmax', name='classification')(dense1_classification)\n\n# Model for segmentation\nnum_classes_segmentation = 1  # Binary segmentation\n\n# Segmentation input\ninputs_segmentation = Input(shape=input_shape, name='segmentation_input')\n\n# Encoder\nconv1_segmentation = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs_segmentation)\npool1_segmentation = MaxPooling2D(pool_size=(2, 2))(conv1_segmentation)\n\n# Decoder\nup1_segmentation = UpSampling2D(size=(2, 2))(pool1_segmentation)\ndecoded_segmentation = Conv2D(num_classes_segmentation, (3, 3), activation='sigmoid', padding='same', name='segmentation')(up1_segmentation)\n\n# Combine both branches\ncombined_model = Model(inputs=[inputs_classification, inputs_segmentation], outputs=[outputs_classification, decoded_segmentation])\n\n# Use appropriate loss functions and metrics for each branch\ncombined_model.compile(optimizer=Adam(),\n                       loss={'classification': 'categorical_crossentropy', 'segmentation': 'binary_crossentropy'},\n                       metrics={'classification': CategoricalAccuracy(), 'segmentation': MeanIoU(num_classes=2)})\n\n# Train the combined model\ncombined_model.fit(\n    {'classification_input': images_train, 'segmentation_input': images_train},\n    {'classification': labels_train, 'segmentation': masks_train},\n    validation_data=(\n        {'classification_input': images_val, 'segmentation_input': images_val},\n        {'classification': labels_val, 'segmentation': masks_val}\n    ),\n    epochs=10, batch_size=32\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-11T17:49:36.203519Z","iopub.execute_input":"2023-12-11T17:49:36.203998Z","iopub.status.idle":"2023-12-11T18:37:08.993234Z","shell.execute_reply.started":"2023-12-11T17:49:36.203959Z","shell.execute_reply":"2023-12-11T18:37:08.990853Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/10\n251/251 [==============================] - 265s 1s/step - loss: 2.0711 - classification_loss: 1.5970 - segmentation_loss: 0.4741 - classification_categorical_accuracy: 0.6595 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.3307 - val_classification_loss: 0.9215 - val_segmentation_loss: 0.4092 - val_classification_categorical_accuracy: 0.6710 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 2/10\n251/251 [==============================] - 261s 1s/step - loss: 1.2282 - classification_loss: 0.8308 - segmentation_loss: 0.3974 - classification_categorical_accuracy: 0.6936 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.2339 - val_classification_loss: 0.8348 - val_segmentation_loss: 0.3991 - val_classification_categorical_accuracy: 0.6940 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 3/10\n251/251 [==============================] - 290s 1s/step - loss: 1.1269 - classification_loss: 0.7448 - segmentation_loss: 0.3821 - classification_categorical_accuracy: 0.7287 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.2396 - val_classification_loss: 0.8590 - val_segmentation_loss: 0.3806 - val_classification_categorical_accuracy: 0.7034 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 4/10\n251/251 [==============================] - 268s 1s/step - loss: 1.0454 - classification_loss: 0.6713 - segmentation_loss: 0.3741 - classification_categorical_accuracy: 0.7547 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.1348 - val_classification_loss: 0.7621 - val_segmentation_loss: 0.3727 - val_classification_categorical_accuracy: 0.7319 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 5/10\n251/251 [==============================] - 263s 1s/step - loss: 0.9639 - classification_loss: 0.5981 - segmentation_loss: 0.3657 - classification_categorical_accuracy: 0.7856 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.1990 - val_classification_loss: 0.8310 - val_segmentation_loss: 0.3680 - val_classification_categorical_accuracy: 0.7184 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 6/10\n251/251 [==============================] - 266s 1s/step - loss: 0.9022 - classification_loss: 0.5401 - segmentation_loss: 0.3621 - classification_categorical_accuracy: 0.8063 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.1532 - val_classification_loss: 0.7851 - val_segmentation_loss: 0.3681 - val_classification_categorical_accuracy: 0.7299 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 7/10\n251/251 [==============================] - 265s 1s/step - loss: 0.8281 - classification_loss: 0.4694 - segmentation_loss: 0.3588 - classification_categorical_accuracy: 0.8300 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.1237 - val_classification_loss: 0.7573 - val_segmentation_loss: 0.3665 - val_classification_categorical_accuracy: 0.7284 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 8/10\n251/251 [==============================] - 269s 1s/step - loss: 0.7823 - classification_loss: 0.4246 - segmentation_loss: 0.3577 - classification_categorical_accuracy: 0.8532 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.2100 - val_classification_loss: 0.8474 - val_segmentation_loss: 0.3626 - val_classification_categorical_accuracy: 0.6990 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 9/10\n251/251 [==============================] - 271s 1s/step - loss: 0.6998 - classification_loss: 0.3430 - segmentation_loss: 0.3568 - classification_categorical_accuracy: 0.8842 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.1937 - val_classification_loss: 0.8311 - val_segmentation_loss: 0.3626 - val_classification_categorical_accuracy: 0.7319 - val_segmentation_mean_io_u_2: 0.3639\nEpoch 10/10\n251/251 [==============================] - 288s 1s/step - loss: 0.6375 - classification_loss: 0.2826 - segmentation_loss: 0.3548 - classification_categorical_accuracy: 0.9051 - segmentation_mean_io_u_2: 0.3676 - val_loss: 1.2417 - val_classification_loss: 0.8769 - val_segmentation_loss: 0.3649 - val_classification_categorical_accuracy: 0.7179 - val_segmentation_mean_io_u_2: 0.3639\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x78fdb91ca650>"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\n\n# Set your paths for testing data\ntest_path = '/kaggle/input/cs770-final-project-skin-lesion-segmentation/Project_Data/Test'\n\n# Load ground truth information for testing\nground_truth_test = pd.read_csv('/kaggle/input/cs770-final-project-skin-lesion-segmentation/Project_Data/Test/ISIC2018_Task3_Test_GroundTruth/ISIC2018_Task3_Test_GroundTruth.csv')\n\n# Load Classification Test Images and Labels\nimages_test_classification = [img_to_array(load_img(os.path.join(test_path, 'ISIC2018_Task3_Test_Input', img + '.jpg'), target_size=(128, 128))) for img in ground_truth_test['image']]\nlabels_test_classification = ground_truth_test[['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']].values\n\n# Convert to numpy arrays and normalize\nimages_test_classification = np.array(images_test_classification) / 255.0\n\n# Directory path for segmentation images and masks\ntest_path_segmentation_images = '/kaggle/input/cs770-final-project-skin-lesion-segmentation/Project_Data/Test/ISBI2016_ISIC_Part1_Test_Data'\ntest_path_segmentation_masks = '/kaggle/input/cs770-final-project-skin-lesion-segmentation/Project_Data/Test/ISBI2016_ISIC_Part1_Test_GroundTruth'\n\n# Get all .jpg files in the segmentation image and masks directory\nsegmentation_image = [f for f in os.listdir(test_path_segmentation_images) if f.endswith('.jpg')]\nsegmentation_mask = [f for f in os.listdir(test_path_segmentation_masks) if f.endswith('.png')]\n\n# Sort the lists of image and mask filenames\nsegmentation_image.sort()\nsegmentation_mask.sort()\n\n# Load the segmentation images and masks\nimages_test_segmentation = [img_to_array(load_img(os.path.join(test_path_segmentation_images, img), target_size=(128, 128))) for img in segmentation_image]\nmasks_test_segmentation_RGB = [img_to_array(load_img(os.path.join(test_path_segmentation_masks, img), target_size=(128, 128))) for img in segmentation_mask]\nmasks_test_segmentation = [cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis] for mask in masks_test_segmentation_RGB]\n\n# Take the first 379 images\nimages_test_classification = images_test_classification[:379]\n\n# Convert to numpy arrays and normalize\nimages_test_segmentation = np.array(images_test_segmentation) / 255.0\nmasks_test_segmentation = np.array(masks_test_segmentation) / 255.0\n\n# Make predictions on the classification test data\nclassification_predictions,segmentation_predictions = combined_model.predict(\n    [images_test_classification, images_test_segmentation]\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:37:16.414591Z","iopub.execute_input":"2023-12-11T18:37:16.415680Z","iopub.status.idle":"2023-12-11T18:38:03.293975Z","shell.execute_reply.started":"2023-12-11T18:37:16.415613Z","shell.execute_reply":"2023-12-11T18:38:03.292508Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"12/12 [==============================] - 3s 229ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the validation set\nval_metrics = combined_model.evaluate(\n    {'classification_input': images_val, 'segmentation_input': images_val},\n    {'classification': labels_val, 'segmentation': masks_val},\n    batch_size=32\n)\n\n# Print the final metrics\nprint(\"Final Classification Accuracy:\", val_metrics[3])  # Adjust the index if needed\nprint(\"Final Segmentation Mean IoU:\", val_metrics[4])  # Adjust the index if needed\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:38:09.705409Z","iopub.execute_input":"2023-12-11T18:38:09.706173Z","iopub.status.idle":"2023-12-11T18:38:29.040129Z","shell.execute_reply.started":"2023-12-11T18:38:09.706112Z","shell.execute_reply":"2023-12-11T18:38:29.038199Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"63/63 [==============================] - 16s 259ms/step - loss: 1.2417 - classification_loss: 0.8769 - segmentation_loss: 0.3649 - classification_categorical_accuracy: 0.7179 - segmentation_mean_io_u_2: 0.3639\nFinal Classification Accuracy: 0.717923104763031\nFinal Segmentation Mean IoU: 0.3638749420642853\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T18:38:41.625093Z","iopub.execute_input":"2023-12-11T18:38:41.626049Z","iopub.status.idle":"2023-12-11T18:38:41.681859Z","shell.execute_reply.started":"2023-12-11T18:38:41.625986Z","shell.execute_reply":"2023-12-11T18:38:41.680059Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n classification_input (Inpu  [(None, 128, 128, 3)]        0         []                            \n tLayer)                                                                                          \n                                                                                                  \n conv2d_2 (Conv2D)           (None, 128, 128, 32)         896       ['classification_input[0][0]']\n                                                                                                  \n segmentation_input (InputL  [(None, 128, 128, 3)]        0         []                            \n ayer)                                                                                            \n                                                                                                  \n max_pooling2d_2 (MaxPoolin  (None, 64, 64, 32)           0         ['conv2d_2[0][0]']            \n g2D)                                                                                             \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 128, 128, 32)         896       ['segmentation_input[0][0]']  \n                                                                                                  \n flatten_1 (Flatten)         (None, 131072)               0         ['max_pooling2d_2[0][0]']     \n                                                                                                  \n max_pooling2d_3 (MaxPoolin  (None, 64, 64, 32)           0         ['conv2d_3[0][0]']            \n g2D)                                                                                             \n                                                                                                  \n dense_1 (Dense)             (None, 128)                  1677734   ['flatten_1[0][0]']           \n                                                          4                                       \n                                                                                                  \n up_sampling2d_1 (UpSamplin  (None, 128, 128, 32)         0         ['max_pooling2d_3[0][0]']     \n g2D)                                                                                             \n                                                                                                  \n classification (Dense)      (None, 7)                    903       ['dense_1[0][0]']             \n                                                                                                  \n segmentation (Conv2D)       (None, 128, 128, 1)          289       ['up_sampling2d_1[0][0]']     \n                                                                                                  \n==================================================================================================\nTotal params: 16780328 (64.01 MB)\nTrainable params: 16780328 (64.01 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}]}]}